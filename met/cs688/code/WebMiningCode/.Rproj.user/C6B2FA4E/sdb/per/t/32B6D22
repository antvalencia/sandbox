{
    "collab_server" : "",
    "contents" : "# Module 4 Code\n\n### --- Example 1: Analyze WebSPHINX results. --------\nlibrary(XML)\ndir <-file.path(paste0('.\\\\Crawlers\\\\','NHL\\\\'))\nHTML.dataset <- list.files(dir,pattern =\"html\") # List of all saved HTML files @ location \"dir\"\n\n# Function to strip the table data from the HTML files\nsieve.HTML <- function(URL) {\n  table <- readHTMLTable(URL) # Read HTML table into a list\n}\n\ntemp.HTML.text <- lapply(as.list( paste0(dir,HTML.dataset)),function(x) sieve.HTML(x)) # Get all the text from the saved HTMLs\n\nquery <- \"Boston Bruins\"\ntemp <- grep(query, temp.HTML.text[[1]][[1]]$Champion)\n# [1]  5 51 53 82 84 94\ntemp.HTML.text[[1]][[1]]$Season[temp]\n\n\n### --- Example 2: Retrieving Financial Data from EDGAR. --------\n# Examples of how to use access data from EDGAR\nrm(list=ls()); cat(\"\\014\") # clear all\nlibrary(\"edgarWebR\")\n\nticker <- \"FRO\" # Company Ticker\nres <- company_information(ticker)\nknitr::kable(res[,1:8],\n             digits = 2,\n             format.args = list(big.mark = \",\"))\nknitr::kable(res[,9:16],\n             digits = 2,\n             format.args = list(big.mark = \",\"))\n\n# Get Latest Submissions\nfilings <- company_filings(ticker, count = 100)\nhead(filings)\ninitial_count <- nrow(filings)\nunique(filings$type) # All the filings pulled\n\nknitr::kable(filings[1, c(\"type\", \"filing_date\", \"accession_number\", \"size\", \"href\")],\n             col.names = c(\"Type\", \"Filing Date\", \"Accession No.\", \"Size\", \"Link\"),\n             digits = 2,\n             format.args = list(big.mark = \",\"))\n\n\n# Get the Latest Complete Submission File\ndocs <- filing_documents(filings$href[1])\nnames(docs)\ndocs$href\n# Parse the Latest Submission File\ndoc <- parse_filing(docs$href[3], include.raw = TRUE)\nhead(doc$text,10)\n\n\n### --- Example 3: Access Current Movies Showtimes. --------\nlibrary(\"rvest\")\nmovies <- read_html(\"http://www.coolidge.org/showtimes/\")\ntitles <- html_nodes(movies, \n                     \"#view-id-external_showtime_date_browser-page div.film-event-title\")\nhtml_text(titles) # Display Current Movies\n# [1] \"Eye in the Sky \"          \"Hello, My Name is Doris \" \"Midnight Special \"        \"The Clan \"  \n\n\n### --- Example 4: Access XML File. --------\n## sequencing.xml file content\n# <data>\n#   <sequence id = \"ancestralSequence\"> \n#   <taxon id=\"test1\">Taxon1\n# </taxon>       \n#   GCAGTTGACACCCTT\n# </sequence>\n#   <sequence id = \"currentSequence\"> \n#   <taxon id=\"test2\">Taxon2\n# </taxon>       \n#   GACGGCGCGGAccAG\n# </sequence>\n#   </data>\n\npth <- file.path(\"c:\", \"Users\", \"ZlatkoFCX\",\"Documents\",\"My Files\",\"Teachng\",\"2016\",\"CS688\",\"Web Scrapping\",\"Scrapping\") # Set Your Path\nlibrary(XML)\n# read XML File located in folder \"pth\"\nx = xmlParse(file.path(pth,\"sequencing.xml\"))\n\n# returns a *list* of text nodes under \"sequence\" tag\nnodeSet = xpathApply(x,\"//sequence/text()\")\n\n# loop over the list returned, and get and modify the node value:\nzz<-sapply(nodeSet,function(G){\n  text = paste(\"Ggg\",xmlValue(G),\"CCaaTT\",sep=\"\")\n  text = gsub(\"[^A-Z]\",\"\",text)\n  xmlValue(G) = text\n})\n\n\n### --- Example 5: Access XML File. --------\n\n\n### --- Example 6: Web Mining News. --------\nlibrary(\"tm\")\nlibrary(tm.plugin.webmining) # Framework for text mining.\nresult <- WebCorpus(GoogleNewsSource(\"Web Analytics\"))\n\nlibrary(\"tm\")\nlibrary(\"tm.plugin.webmining\")\ngooglenews <- WebCorpus(GoogleNewsSource(\"US economy\", since=\"1-1-2015\", until=\"31-1-2015\"))\n\n\n### --- Example 7: Shiny YahooNewsSource App. --------\n# Save these 2 files separately in the same folder\n# Example: Shiny app that search \"YahooNewsSource\" for a keyword that we specify\n# server.R \nlibrary(shiny)\nlibrary(tm)\nlibrary(tm.plugin.webmining)\n# Define server logic required to implement search\nshinyServer(function(input, output) {\n  output$text1 <- renderUI({ \n    Str1 <- paste(\"You have selected:\", input$select)\n    Str2 <- paste(\"and searched for:\", input$text.Search)\n    result <- WebCorpus(YahooNewsSource(input$text.Search))\n    dataOutput <- paste(\"<li>\",strong(meta(result[[1]])$heading),\"</li>\") # Get the first result\n    # Str3 <- paste(\"Search Results:\", dataInput)\n    HTML(paste(Str1, Str2, \"Search Results:\", dataOutput, sep = '<br/>'))\n  })\n})\n\n# Example: Shiny app that search \"YahooNewsSource\" for a specify search query \n# ui.R \nlibrary(shiny)\n\n# Define UI for application \nshinyUI(fluidPage(\n  titlePanel(\"News Search App\"),   # Application title (Panel 1)\n  \n  sidebarLayout(    \t\t# Widget (Panel 2)\n    sidebarPanel(h3(\"Search panel\"),\n                 # Search for\n                 textInput(\"text.Search\", label = h5(\"Search for\"), \n                           value = \" Web Analytics\"),                 \n                 # Where to search \n                 selectInput(\"select\",\n                             label = h5(\"Choose Data Source\"),\n                             choices = c(\"YahooNewsSource\", \"GoogleNewsSource\"),\n                             selected = \"Yahoo! News\"),\n                 # Start Search\n                 submitButton(\"Results\")\n    ),\n    # Display Panel (Panel 3)\n    mainPanel(                   \n      h1(\"Display Panel\",align = \"center\"),\n      htmlOutput(\"text1\")\n    )\n  )\n))\n\n\n### --- Example 8: Search Wikipedia web pages. --------\n# Save these 3 files separately in the same folder (Related to HW#4)\n# Example: Shiny app that search Wikipedia web pages\n# File: ui.R \nlibrary(shiny)\ntitles <- c(\"Web_analytics\",\"Text_mining\",\"Integral\", \"Calculus\", \n            \"Lists_of_integrals\", \"Derivative\",\"Alternating_series\",\n            \"Pablo_Picasso\",\"Vincent_van_Gogh\",\"Lev_Tolstoj\",\"Web_crawler\")\n# Define UI for application \nshinyUI(fluidPage(\n  # Application title (Panel 1)\n  titlePanel(\"Wiki Pages\"), \n  # Widget (Panel 2) \n  sidebarLayout(\n    sidebarPanel(h3(\"Search panel\"),\n                 # Where to search \n                 selectInput(\"select\",\n                             label = h5(\"Choose from the following Wiki Pages on\"),\n                             choices = titles,\n                             selected = titles, multiple = TRUE),\n                 # Start Search\n                 submitButton(\"Results\")\n    ),\n    # Display Panel (Panel 3)\n    mainPanel(                   \n      h1(\"Display Panel\",align = \"center\"),\n      plotOutput(\"distPlot\")\n    )\n  )\n))\n\n\n# Example: Shiny app that search Wikipedia web pages\n# File: server.R \nlibrary(shiny)\nlibrary(tm)\nlibrary(stringi)\nlibrary(proxy)\nsource(\"WikiSearch.R\")\n\nshinyServer(function(input, output) {\n  output$distPlot <- renderPlot({ \n    result <- SearchWiki(input$select)\n    plot(result, labels = input$select, sub = \"\",main=\"Wikipedia Search\")\n  })\n})\n\n\n# Example: Shiny app that search Wikipedia web pages\n# File: WikiSearch.R\n\n# Wikipedia Search\nlibrary(tm)\nlibrary(stringi)\nlibrary(WikipediR)\n# library(proxy)\n\nSearchWiki <- function (titles) {\n  # wiki.URL <- \"https://en.wikipedia.org/wiki/\"\n  # articles <- lapply(titles,function(i) stri_flatten(readLines(stri_paste(wiki.URL,i)), col = \" \"))\n  \n  articles <- lapply(titles,function(i) page_content(\"en\",\"wikipedia\", page_name = i,as_wikitext=TRUE)$parse$wikitext)\n  \n  docs <- Corpus(VectorSource(articles)) # Get Web Pages' Corpus\n  remove(articles)\n  \n  # Text analysis - Preprocessing \n  transform.words <- content_transformer(function(x, from, to) gsub(from, to, x))\n  temp <- tm_map(docs, transform.words, \"<.+?>\", \" \")\n  temp <- tm_map(temp, transform.words, \"\\t\", \" \")\n  temp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase\n  temp <- tm_map(temp, PlainTextDocument)\n  temp <- tm_map(temp, stripWhitespace)\n  temp <- tm_map(temp, removeWords, stopwords(\"english\"))\n  temp <- tm_map(temp, removePunctuation)\n  temp <- tm_map(temp, stemDocument, language = \"english\") # Perform Stemming\n  remove(docs)\n  \n  # Create Dtm \n  dtm <- DocumentTermMatrix(temp)\n  dtm <- removeSparseTerms(dtm, 0.4)\n  dtm$dimnames$Docs <- titles\n  docsdissim <- dist(as.matrix(dtm), method = \"euclidean\") # Distance Measure\n  h <- hclust(as.dist(docsdissim), method = \"ward.D2\") # Group Results\n}\n\n### --- Example 9: Search Reuters documents. --------\nreut21578 <- system.file(\"texts\", \"crude\", package = \"tm\") # Reuters Files Location\nreuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain)) # Get Corpus\n# Text analysis - Preprocessing with tm package functionality\ntemp  <- reuters \ntransform.words <- content_transformer(function(x, from, to) gsub(from, to, x))\ntemp <- tm_map(temp, content_transformer(tolower)) # Conversion to Lowercase\ntemp <- tm_map(temp, stripWhitespace)\ntemp <- tm_map(temp, removeWords, stopwords(\"english\"))\ntemp <- tm_map(temp, removePunctuation)\n# Create Document Term Matrix \ndtm <- DocumentTermMatrix(temp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1508945904136.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2573187077",
    "id" : "32B6D22",
    "lastKnownWriteTime" : 1521644375,
    "last_content_update" : 1521644375128,
    "path" : "~/My Files/Teachng/2018/Spring/Classroom/Class 5/Code/WebMiningCode/WebMiningCode.R",
    "project_path" : "WebMiningCode.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}