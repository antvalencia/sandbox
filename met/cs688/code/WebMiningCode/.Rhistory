11/(11+10)*100
11/(11+16)*100
P1<-52.4; R1<-40.7
P2<-5.6; R2<-66.7
F1<-2*P1*R1/(P1+R1)
F1
F2<-2*P2*R2/(P2+R2)
F2
getwd()
dir <- file.path(getwd(),"Crawlers" "NHL")
dir <- paste0('.\\Data\\',"Crawlers\\" "NHL")
paste0('.\\Data\\',"Crawlers\\", "NHL")
library(XML)
dir <- paste0('.\\Data\\',"Crawlers\\", "NHL\\")
HTML.dataset <- list.files(dir,pattern ="html")
HTML.dataset
dir
dir <- paste0('.\\Data\\',"Crawlers\\", "NHL")
dir
list.files(dir,pattern ="html")
dir <- paste0('.\\Data\\',"Crawlers\\", "NHL\\*")
list.files(dir,pattern ="html")
?list.files
dir
ir <- paste0('.\\Data\\','Crawlers\\', 'NHL\\')
dir <- paste0('.\\Data\\','Crawlers\\', 'NHL\\')
dir
list.files(dir,pattern ="html")
dir <-'.\\Data\\Crawlers\\NHL\\'
list.files(dir,pattern ="html")
dir <- paste0('.\\Crawlers\\', 'NHL\\')
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
HTML.dataset
sieve.HTML <- function(URL) {
table <- readHTMLTable(URL) # Read HTML table into a list
}
temp.HTML.text <- lapply(HTML.dataset,function(x) sieve.HTML(x)) # Get all the text from the saved HTMLs
temp.HTML.text
HTML.dataset
class(HTML.dataset)
lapply(as.list( HTML.dataset),function(x) sieve.HTML(x))
library(XML)
dir <- paste0('.\\Crawlers\\', 'NHL\\')
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
# Function to strip the table data from the HTML files
sieve.HTML <- function(URL) {
table <- readHTMLTable(URL) # Read HTML table into a list
}
HTML.dataset
dir
dir <-file.path(paste0('.\\Crawlers\\', 'NHL\\'))
HTML.dataset <- list.files(dir,pattern ="html")
HTML.dataset
HTML.dataset <- list.files(dir,pattern ="html")
file.path(dir,pattern ="html")
paste0(dir,HTML.dataset)
paste0(dir,HTML.dataset[[1]])
library(XML)
dir <-file.path(paste0('.\\Crawlers\\', 'NHL\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
# Function to strip the table data from the HTML files
sieve.HTML <- function(URL) {
table <- readHTMLTable(URL) # Read HTML table into a list
}
temp.HTML.text <- lapply(as.list( paste0(dir,HTML.dataset)),function(x) sieve.HTML(x)) # Get all the text from the saved HTMLs
temp.HTML.text
query <- "Boston Bruins"
temp <- grep(query, temp.HTML.text[[1]][[1]]$Champion)
temp
temp.HTML.text[[1]][[1]]$Season[temp]
library(XML)
dir <-file.path(paste0('.\\Crawlers\\','NHL\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
# Function to strip the table data from the HTML files
sieve.HTML <- function(URL) {
table <- readHTMLTable(URL) # Read HTML table into a list
}
temp.HTML.text <- lapply(as.list( paste0(dir,HTML.dataset)),function(x) sieve.HTML(x)) # Get all the text from the saved HTMLs
temp.HTML.text[[1]][[1]]
?dir
dir()
library(XML)
dir <-file.path(paste0('.\\Crawlers\\','EPA\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
HTML.dataset
library(XML)
dir <-file.path(paste0('.\\Crawlers\\','NHL\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
HTML.dataset
library(XML)
dir <-file.path(paste0('.\\Crawlers\\','EPA\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
library("rvest")
EPA_html <- read_html("/https://www.epa.gov/")
library("rvest")
EPA_html <- read_html("/https://www.epa.gov/")
EPA_html <- read_html("/http://www.epa.gov/")
EPA_html <- read_html("/https://www.epa.gov")
library("rvest")
EPA_html <- read_html("https://www.epa.gov/")
EPA_html
library("rvest")
movies <- read_html("http://www.coolidge.org/showtimes/")
movies
titles <- html_nodes(movies,
"#view-id-external_showtime_date_browser-page div.film-event-title")
titles
html_text(titles)
titles
movies
library("rvest")
movies <- read_html("http://www.coolidge.org/showtimes/")
html_nodes(movies,
"#view-id-external_showtime_date_browser-page div.film-event-title")
library(XML)
dir <-file.path(paste0('.\\Crawlers\\','NHL\\'))
HTML.dataset <- list.files(dir,pattern ="html") # List of all saved HTML files @ location "dir"
# Function to strip the table data from the HTML files
sieve.HTML <- function(URL) {
table <- readHTMLTable(URL) # Read HTML table into a list
}
temp.HTML.text <- lapply(as.list( paste0(dir,HTML.dataset)),function(x) sieve.HTML(x)) # Get all the text from the saved HTMLs
query <- "Boston Bruins"
temp <- grep(query, temp.HTML.text[[1]][[1]]$Champion)
temp.HTML.text[[1]][[1]]$Season[temp]
rm(list=ls()); cat("\014") # clear all
library("edgarWebR")
ticker <- "FRO" # Company Ticker
res <- company_information(ticker)
knitr::kable(res[,1:8],
digits = 2,
format.args = list(big.mark = ","))
knitr::kable(res[,9:16],
digits = 2,
format.args = list(big.mark = ","))
filings <- company_filings(ticker, count = 100)
head(filings)
initial_count <- nrow(filings)
unique(filings$type) # All the filings pulled
knitr::kable(filings[1, c("type", "filing_date", "accession_number", "size", "href")],
col.names = c("Type", "Filing Date", "Accession No.", "Size", "Link"),
digits = 2,
format.args = list(big.mark = ","))
# Get the Latest Complete Submission File
docs <- filing_documents(filings$href[1])
names(docs)
docs$href
doc <- parse_filing(docs$href[3], include.raw = TRUE)
head(doc$text,10)
