# KNN Classification of SMS Dataset
rm(list=ls()); cat("\014") # clear all
library(tm) # Use this package for Text Mining
load("Data/SMS_DTM.RData") # Load dtm from saved data
dtm <- as.matrix(dtm)
dtm <- dtm[1:1000,] # Subset DTM
# Split the Document-Term Matrix into Train & Test Datasets
library(class) #
# Consider "spam" as Positive and "ham" as Negative
Positive <- "spam"; Negative <- "ham"; CM.Names <- c(Positive,Negative)
DS.Size <- dim(dtm)[1]
Test.Train.Percent <- 0.6 # Split Data into 60% for Training and 40% for Testing
ix.Range <- round(DS.Size*Test.Train.Percent)
train.Range <- seq(from = 1, to = ix.Range, by = 1); test.Range <- seq(from = (ix.Range+1), to = DS.Size, by = 1)
train.doc <- dtm[train.Range,] # Dataset for which classification is already known
test.doc <- dtm[test.Range,] # Dataset we are trying to classify
# Generate TAGS - Correct answers for the Train dataset
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive) # 446 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 2898 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Train <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
# Generate TAGS - Correct answers for the Test dataset
z <- c(SMS.Data$Tag[test.Range]);
sum(z==Positive) # 381 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 2406 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Test <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
sum(z==Positive)
sum(z==Negative)
dim(dtm)
# Consider "spam" as Positive and "ham" as Negative
Positive <- "spam"; Negative <- "ham"; CM.Names <- c(Positive,Negative)
DS.Size <- dim(dtm)[1]
Test.Train.Percent <- 0.6 # Split Data into 60% for Training and 40% for Testing
ix.Range <- round(DS.Size*Test.Train.Percent)
train.Range <- seq(from = 1, to = ix.Range, by = 1); test.Range <- seq(from = (ix.Range+1), to = DS.Size, by = 1)
train.Range
train.doc <- dtm[train.Range,] # Dataset for which classification is already known
test.doc <- dtm[test.Range,] # Dataset we are trying to classify
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive) # 446 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 2898 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Train <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
sum(z==Positive)
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive)
sum(z==Negative)
train.Range
z
sum(z=="spam")
sum(z=="ham")
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive) # 88 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 512 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Train <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
test.Range
z <- c(SMS.Data$Tag[test.Range]);
sum(z==Positive) # 381 - Number of Positive events ("spam") in the Test Data
sum(z==Negative)
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Test <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
set.seed(0)
prob.test <- knn(train.doc, test.doc, Tags.Train, k = 2, prob=TRUE) # k-number of neighbors considered
# Display Classification Results
a <- 1:length(prob.test)
b <- levels(prob.test)[prob.test] # asign your classification Tags (Talk or Sci)
c <- attributes(prob.test)$prob # asign your classification probability values
d <- prob.test==Tags.Test # Logicaly match your classification Tags with the known "Tags"
result <- data.frame(Doc=a,Predict=b,Prob=c, Correct=d) # Tabulate your result
sum(d)/length(Tags.Test) # % Correct Classification (0.86)
# KNN Classification of SMS Dataset
rm(list=ls()); cat("\014") # clear all
library(tm) # Use this package for Text Mining
load("Data/SMS_DTM.RData") # Load dtm from saved data
dtm <- as.matrix(dtm)
dtm <- dtm[1:1000,] # Subset DTM
# Split the Document-Term Matrix into Train & Test Datasets
library(class) #
# Consider "spam" as Positive and "ham" as Negative
Positive <- "spam"; Negative <- "ham"; CM.Names <- c(Positive,Negative)
DS.Size <- dim(dtm)[1]
Test.Train.Percent <- 0.6 # Split Data into 60% for Training and 40% for Testing
ix.Range <- round(DS.Size*Test.Train.Percent)
train.Range <- seq(from = 1, to = ix.Range, by = 1); test.Range <- seq(from = (ix.Range+1), to = DS.Size, by = 1)
train.doc <- dtm[train.Range,] # Dataset for which classification is already known
test.doc <- dtm[test.Range,] # Dataset we are trying to classify
# Generate TAGS - Correct answers for the Train dataset
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive) # 88 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 512 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Train <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
# Generate TAGS - Correct answers for the Test dataset
z <- c(SMS.Data$Tag[test.Range]);
sum(z==Positive) # 64 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 336 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Test <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
# 1) KNN Classificationusing package "class" ====
set.seed(0)
prob.test <- knn(train.doc, test.doc, Tags.Train, k = 2, prob=TRUE) # k-number of neighbors considered
### -----------
# Display Classification Results
a <- 1:length(prob.test)
b <- levels(prob.test)[prob.test] # asign your classification Tags (Talk or Sci)
c <- attributes(prob.test)$prob # asign your classification probability values
d <- prob.test==Tags.Test # Logicaly match your classification Tags with the known "Tags"
result <- data.frame(Doc=a,Predict=b,Prob=c, Correct=d) # Tabulate your result
sum(d)/length(Tags.Test) # % Correct Classification (0.86)
# Insert your code to find:
# Confusion Matrix
# Precision
# Recall
# Fscore
# Accuracy <- (TP+TN)/(TP+TN+FP+FN)
result
result$Predict == 'Positive'
result[result$Predict == 'Positive']
result[which(result$Predict == 'Positive'), ]
result[which(result$Predict == 'Positive') && which(result$Correct), ]
result[which(result$Predict == 'Positive') && which(result$Correct == TRUE), ]
result[which(result$Predict == 'Positive') && which(result$Correct == 'TRUE'), ]
result[which(result$Predict == 'Positive') & which(result$Correct == 'TRUE'), ]
result[which(result$Predict == 'Positive') & which(result$Correct == TRUE), ]
result[which(result$Predict == 'Positive' & result$Correct == TRUE), ]
nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
nrow(result[which(result$Predict != 'Negative' & result$Predict != 'Positive' ), ])
TP <- nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
FP <- nrow(result[which(result$Predict == 'Positive' & result$Correct == FALSE), ])
TN <- nrow(result[which(result$Predict == 'Negative' & result$Correct == TRUE), ])
FP <- nrow(result[which(result$Predict == 'Negative' & result$Correct == FALSE), ])
TP <- nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
FP <- nrow(result[which(result$Predict == 'Positive' & result$Correct == FALSE), ])
TN <- nrow(result[which(result$Predict == 'Negative' & result$Correct == TRUE), ])
FN <- nrow(result[which(result$Predict == 'Negative' & result$Correct == FALSE), ])
tp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
fp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == FALSE), ])
tn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == TRUE), ])
fn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == FALSE), ])
# just making sure
nrow(result[which(result$Predict != 'Negative' & result$Predict != 'Positive' ), ])
# returns 0
# Precision
precision <- tp/(tp+fp)
# Recall
recall <- tp/(tp+fn)
# Fscore
f_score <- 2*((precision*recall)/(precision+recall))
# Accuracy <- (TP+TN)/(TP+TN+FP+FN)
accuracy <- (tp+tn)/(tp+tn+fp+fn)
# Insert your code to find:
# Confusion Matrix
tp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
fp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == FALSE), ])
tn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == TRUE), ])
fn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == FALSE), ])
# just making sure
nrow(result[which(result$Predict != 'Negative' & result$Predict != 'Positive' ), ])
# returns 0
# Precision
precision <- tp/(tp+fp)
precision
# Recall
recall <- tp/(tp+fn)
recall
# Fscore
f_score <- 2*((precision*recall)/(precision+recall))
f_score
# Accuracy <- (TP+TN)/(TP+TN+FP+FN)
accuracy <- (tp+tn)/(tp+tn+fp+fn)
accuracy
result
dtm
head(dtm)
head(SMS.Data)
# KNN Classification of SMS Dataset
rm(list=ls()); cat("\014") # clear all
library(tm) # Use this package for Text Mining
load("Data/SMS_DTM.RData") # Load dtm from saved data
dtm <- as.matrix(dtm)
dtm <- dtm[1:1000,] # Subset DTM
# Split the Document-Term Matrix into Train & Test Datasets
library(class) #
# Consider "spam" as Positive and "ham" as Negative
Positive <- "spam"; Negative <- "ham"; CM.Names <- c(Positive,Negative)
DS.Size <- dim(dtm)[1]
Test.Train.Percent <- 0.6 # Split Data into 60% for Training and 40% for Testing
ix.Range <- round(DS.Size*Test.Train.Percent)
train.Range <- seq(from = 1, to = ix.Range, by = 1); test.Range <- seq(from = (ix.Range+1), to = DS.Size, by = 1)
train.doc <- dtm[train.Range,] # Dataset for which classification is already known
test.doc <- dtm[test.Range,] # Dataset we are trying to classify
# Generate TAGS - Correct answers for the Train dataset
z <- c(SMS.Data$Tag[train.Range])
sum(z==Positive) # 88 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 512 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Train <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
# Generate TAGS - Correct answers for the Test dataset
z <- c(SMS.Data$Tag[test.Range]);
sum(z==Positive) # 64 - Number of Positive events ("spam") in the Test Data
sum(z==Negative) # 336 - Number of Negative events ("ham") in the Test Data
z <- gsub(Positive,"Positive",z); z <- gsub(Negative,"Negative",z) # Replace Tags with
Tags.Test <- factor(z, levels = c("Positive","Negative")) # kNN expects Tags to be Factors data type
# 1) KNN Classificationusing package "class" ====
set.seed(0)
prob.test <- knn(train.doc, test.doc, Tags.Train, k = 2, prob=TRUE) # k-number of neighbors considered
### -----------
# Display Classification Results
a <- 1:length(prob.test)
b <- levels(prob.test)[prob.test] # asign your classification Tags (Talk or Sci)
c <- attributes(prob.test)$prob # asign your classification probability values
d <- prob.test==Tags.Test # Logicaly match your classification Tags with the known "Tags"
result <- data.frame(Doc=a,Predict=b,Prob=c, Correct=d) # Tabulate your result
sum(d)/length(Tags.Test) # % Correct Classification (0.86)
result
# Insert your code to find:
# Confusion Matrix
tp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == TRUE), ])
fp <- nrow(result[which(result$Predict == 'Positive' & result$Correct == FALSE), ])
tn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == TRUE), ])
fn <- nrow(result[which(result$Predict == 'Negative' & result$Correct == FALSE), ])
# just making sure
nrow(result[which(result$Predict != 'Negative' & result$Predict != 'Positive' ), ])
# returns 0
# Precision
precision <- tp/(tp+fp)
precision
# Recall
recall <- tp/(tp+fn)
recall
# Fscore
f_score <- 2*((precision*recall)/(precision+recall))
f_score
# Accuracy <- (TP+TN)/(TP+TN+FP+FN)
accuracy <- (tp+tn)/(tp+tn+fp+fn)
accuracy
n.event <- 1
n.trials <- 150
success.p <- 0.33
dbinom(n.event, size=n.trials, prob=success.p)
y <- dbinom(n.event, size=n.trials, prob=success.p)
plot(y)
y <- dbinom(size=n.trials, prob=success.p)
plot(y)
n.trials <- 150
success.p <- 0.33
x <- seq(from=0, to=n.trials, by=1)
y <- dbinom(x, size=n.trials, prob=success.p)
plot(x, y)
n.trials <- 150
success.p <- 0.33
x <- seq(from=0, to=n.trials, by=1)
y <- dbinom(x, size=n.trials, prob=success.p)
plot(x, y)
title("Binomial probability distribution")
cor(mtcars$mpg, mtcars$hp)
cor.test(mtcars$mpg, mtcars$hp)
Y <- mtcars$mpg
#            - take X to be factor(mtcars$cyl)
X <- actor(mtcars$cyl)
#         b. To get the p-value, you would need to execute the following code
#            - model <- aov(Y~X)
model <- aov(Y~X)
#            - summary(model)
summary(model)
Y <- mtcars$mpg
#            - take X to be factor(mtcars$cyl)
X <- factor(mtcars$cyl)
#         b. To get the p-value, you would need to execute the following code
#            - model <- aov(Y~X)
model <- aov(Y~X)
#            - summary(model)
summary(model)
Y <- mtcars$hp
#     b. X <- factor(mtcars$wt)
X <- factor(mtcars$wt)
#     c. Based on the p-value, write a clear comment in your code if the null hypothesis has to be kept or rejected.
#            - model <- aov(Y~X)
model <- aov(Y~X)
#            - summary(model)
summary(model)
setwd("~/misc/code/met/cs699/hw/03")
# Problem 1 (20 points)
# Consider the following dataset (sorted in non-decreasing order):
#   <10, 12, 16, 16, 29, 32, 51, 60, 60, 66, 70, 72, 87, 96, 120>
ds <- c(10, 12, 16, 16, 29, 32, 51, 60, 60, 66, 70, 72, 87, 96, 120)
# (1) Perform the equal width binning on the above data with 3 bins.
#   Note that the bin boundaries are integers in the textbook and in
#   the online lecture module (to make the discussion simple). But,
#   for this assignment your bin boundaries will include fractions.
#   So, you must follow the example in the lecture slides. For each
#   bin, show:
#     - the bin interval
#     - data values in the bin
#     - smoothed values using:
#         - bin means
#         - bin medians
#         - bin boundaries.
A <- min(ds)
B <- max(ds)
N <- 3
W <- (B - A)/N
I1.min <- A
I1.max <- I1.min + W
I2.min <- I1.max
I2.max <- I2.min + W
I3.min <- I2.max
I3.max <- I3.min + W
sprintf("Bin 1 interval: [%f, %f)", I1.min, I1.max)
sprintf("Bin 2 interval: [%f, %f)", I2.min, I2.max)
sprintf("Bin 3 interval: [%f, %f]", I3.min, I3.max)
I1.values <- ds[(ds>=I1.min) & (ds<I1.max)]
I2.values <- ds[(ds>=I2.min) & (ds<I2.max)]
I3.values <- ds[(ds>=I3.min) & (ds<=I3.max)]
print("Bin 1 values:")
print(I1.values)
print("Bin 2 values:")
print(I2.values)
print("Bin 3 values:")
print(I3.values)
print("Bin 1 means:")
print(rep(mean(I1.values), length(I1.values)))
print("Bin 2 means:")
print(rep(mean(I2.values), length(I2.values)))
print("Bin 3 means:")
print(rep(mean(I3.values), length(I3.values)))
print("Bin 1 medians:")
print(rep(median(I1.values), length(I1.values)))
print("Bin 2 medians:")
print(rep(median(I2.values), length(I2.values)))
print("Bin 3 medians:")
print(rep(median(I3.values), length(I3.values)))
I1.fn <- function(arg) {
rng <- c(min(I1.values), max(I1.values))
result <- rng[which.min(abs(rng - arg))]
}
I1.boundaries <- unlist(lapply(I1.values, FUN=I1.fn), recursive=FALSE)
I2.fn <- function(arg) {
rng <- c(min(I2.values), max(I2.values))
result <- rng[which.min(abs(rng - arg))]
}
I2.boundaries <- unlist(lapply(I2.values, FUN=I2.fn), recursive=FALSE)
I3.fn <- function(arg) {
rng <- c(min(I3.values), max(I3.values))
result <- rng[which.min(abs(rng - arg))]
}
I3.boundaries <- unlist(lapply(I3.values, FUN=I3.fn), recursive=FALSE)
print("Bin 1 boundaries:")
print(I1.boundaries)
print("Bin 2 boundaries:")
print(I2.boundaries)
print("Bin 3 boundaries:")
print(I3.boundaries)
# (2) Repeat the same with equal depth binning with 3 bins.
ds.mtx <- matrix(ds, ncol=3)
I1.values <- ds.mtx[,1]
I2.values <- ds.mtx[,2]
I3.values <- ds.mtx[,3]
print("Bin 1 values:")
print(I1.values)
print("Bin 2 values:")
print(I2.values)
print("Bin 3 values:")
print(I3.values)
print("Bin 1 means:")
print(rep(mean(I1.values), length(I1.values)))
print("Bin 2 means:")
print(rep(mean(I2.values), length(I2.values)))
print("Bin 3 means:")
print(rep(mean(I3.values), length(I3.values)))
print("Bin 1 medians:")
print(rep(median(I1.values), length(I1.values)))
print("Bin 2 medians:")
print(rep(median(I2.values), length(I2.values)))
print("Bin 3 medians:")
print(rep(median(I3.values), length(I3.values)))
I1.fn <- function(arg) {
rng <- c(min(I1.values), max(I1.values))
result <- rng[which.min(abs(rng - arg))]
}
I1.boundaries <- unlist(lapply(I1.values, FUN=I1.fn), recursive=FALSE)
I2.fn <- function(arg) {
rng <- c(min(I2.values), max(I2.values))
result <- rng[which.min(abs(rng - arg))]
}
I2.boundaries <- unlist(lapply(I2.values, FUN=I2.fn), recursive=FALSE)
I3.fn <- function(arg) {
rng <- c(min(I3.values), max(I3.values))
result <- rng[which.min(abs(rng - arg))]
}
I3.boundaries <- unlist(lapply(I3.values, FUN=I3.fn), recursive=FALSE)
print("Bin 1 boundaries:")
print(I1.boundaries)
print("Bin 2 boundaries:")
print(I2.boundaries)
print("Bin 3 boundaries:")
print(I3.boundaries)
# (3) If you transform the dataset into the interval of [0, 1] using
#   Min-max normalization, what is the new value of 51?
V <- 51
new_min <- 0
new_max <- 1
V.min.max.norm <- ((V - min(ds))/(max(ds) - min(ds)))*(new_max - new_min) + new_min
print(V.min.max.norm)
# (4) If you transform the dataset using z-score normalization using
#   the standard deviation, what is the new value of 51?
mu <- mean(ds)
omega <- sd(ds)
V.z.score.norm <- (V - mu)/omega
print(V.z.score.norm)
# (5) If you transform the dataset using z-score normalization using
#   the mean absolute deviation, what is the new value of 51?
V.z.score.norm.abs <- abs((V - mu)/omega)
print(V.z.score.norm.abs)
# Problem 2 (10 points)
# This problem is a practice of calculating correlations between input
#   attributes (or predictive attributes) and the output attribute
#   (or predictable attribute) in the a3-p2.csv dataset. This dataset
#   has 5 attributes and 100 tuples. The first 4 attributes are input
#   attributes and the last attribute, A5, is the output attribute. Your
#   task is to calculate the correlation between each input attribute and
#   the output attribute. In other words, you are required to calculate the
#   following four correlations:
#
#   correl(A1, A5)
#   correl(A2, A5)
#   correl(A3, A5)
#   correl(A4, A5)
#
#   Here, correl(X, Y) denotes the correlation between X and Y.
#   In your submission, include all four correlations, and indicate the
#   attribute that has the strongest correlation with A5.
dat = read.csv("a3-p2.csv", header = TRUE)
A1.cor <- cor(dat$A1, dat$A5)
A2.cor <- cor(dat$A2, dat$A5)
A3.cor <- cor(dat$A3, dat$A5)
A4.cor <- cor(dat$A4, dat$A5)
print(A1.cor)
print(A2.cor)
print(A3.cor)
print(A4.cor)
# A3 is most strongly correlated
cor.a.b <- function(vec.a, vec.b) {
a.bar <- mean(vec.a)
b.bar <- mean(vec.b)
a.sd <- sd(vec.a)
b.sd <- sd(vec.b)
total <- 0
for(i in 1:length(vec.a)) {
total <- total + ((vec.a[i] - a.bar) * (vec.b[i] - b.bar))
}
result <- total/((length(vec.a)-1) * a.sd * b.sd)
}
A1.cor <- cor.a.b(dat$A1, dat$A5)
A2.cor <- cor.a.b(dat$A2, dat$A5)
A3.cor <- cor.a.b(dat$A3, dat$A5)
A4.cor <- cor.a.b(dat$A4, dat$A5)
print(A1.cor)
print(A2.cor)
print(A3.cor)
print(A4.cor)
# A3 is most strongly correlated
